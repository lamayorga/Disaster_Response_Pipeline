{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import nltk\n",
    "nltk.download(['punkt', 'wordnet', 'averaged_perceptron_tagger', 'stopwords'])\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from database\n",
    "engine = create_engine('sqlite:///InsertDatabaseName.db')\n",
    "df = pd.read_sql_table('InsertTableName',engine)\n",
    "X = df.message.values\n",
    "y = df.iloc[:,4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write tokenization function to process text data\n",
    "def tokenize(text):\n",
    "    \n",
    "    # Normalize text\n",
    "    text = re.sub(r'[^a-zA-Z0-9]',' ',text.lower())\n",
    "\n",
    "    # Token messages\n",
    "    words = word_tokenize(text)\n",
    "    tokens = [w for w in words if w not in stopwords.words(\"english\")]\n",
    "\n",
    "    # Lemmatizer and clean\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        clean_tok = lemmatizer.lemmatize(tok).strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build machine learning pipeline\n",
    "pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# Train pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate metrics of ML pipeline \n",
    "def eval_metrics(ArrayL, ArrayP, col_names):\n",
    "    \n",
    "    \"\"\"Evalute metrics of the ML pipeline model\n",
    "    \n",
    "    inputs:\n",
    "    ArrayL: array. Array containing the real labels.\n",
    "    ArrayP: array. Array containing predicted labels.\n",
    "    col_names: list of strings. List containing names for each of the ArrayP fields.\n",
    "       \n",
    "    Returns:\n",
    "    data_metrics: Contains accuracy, precision, recall \n",
    "    and f1 score for a given set of ArrayL and ArrayP labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    metrics = []\n",
    "    \n",
    "    # Evaluate metrics for each set of labels\n",
    "    for i in range(len(col_names)):\n",
    "        accuracy = accuracy_score(ArrayL[:, i], ArrayP[:, i])\n",
    "        precision = precision_score(ArrayL[:, i], ArrayP[:, i])\n",
    "        recall = recall_score(ArrayL[:, i], ArrayP[:, i])\n",
    "        f1 = f1_score(ArrayL[:, i], ArrayP[:, i])\n",
    "        \n",
    "        metrics.append([accuracy, precision, recall, f1])\n",
    "    \n",
    "    # Store metrics\n",
    "    metrics = np.array(metrics)\n",
    "    data_metrics = pd.DataFrame(data = metrics, index = col_names, columns = ['Accuracy', 'Precision', 'Recall', 'F1'])\n",
    "      \n",
    "    return data_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Accuracy  Precision    Recall        F1\n",
      "related                 0.989601   0.990452  0.995972  0.993204\n",
      "request                 0.986527   0.997428  0.924062  0.959345\n",
      "offer                   0.998873   1.000000  0.750000  0.857143\n",
      "aid_related             0.984068   0.993970  0.967829  0.980725\n",
      "medical_help            0.986732   0.999224  0.833118  0.908642\n",
      "medical_products        0.992879   0.998817  0.859470  0.923919\n",
      "search_and_rescue       0.994570   0.997738  0.807692  0.892713\n",
      "security                0.995594   1.000000  0.759104  0.863057\n",
      "military                0.995338   1.000000  0.861702  0.925714\n",
      "water                   0.994672   0.999136  0.918254  0.956989\n",
      "food                    0.995236   0.997633  0.959909  0.978407\n",
      "shelter                 0.992111   0.999365  0.911407  0.953362\n",
      "clothing                0.998207   1.000000  0.882943  0.937833\n",
      "money                   0.995851   1.000000  0.823913  0.903456\n",
      "missing_people          0.996978   0.994253  0.748918  0.854321\n",
      "refugees                0.993494   0.998110  0.807339  0.892646\n",
      "death                   0.993648   1.000000  0.862528  0.926190\n",
      "other_aid               0.978126   0.996317  0.837786  0.910200\n",
      "infrastructure_related  0.985195   0.999004  0.776917  0.874074\n",
      "transport               0.991957   0.998656  0.826474  0.904443\n",
      "buildings               0.992265   0.998821  0.849549  0.918157\n",
      "electricity             0.996209   1.000000  0.811705  0.896067\n",
      "tools                   0.998053   0.989362  0.715385  0.830357\n",
      "hospitals               0.997131   1.000000  0.735849  0.847826\n",
      "shops                   0.998463   1.000000  0.690722  0.817073\n",
      "aid_centers             0.996773   1.000000  0.731915  0.845209\n",
      "other_infrastructure    0.990625   1.000000  0.789655  0.882466\n",
      "weather_related         0.987398   0.996011  0.958851  0.977078\n",
      "floods                  0.991752   0.999311  0.900683  0.947437\n",
      "storm                   0.994109   0.997687  0.939542  0.967742\n",
      "fire                    0.997746   1.000000  0.785366  0.879781\n",
      "earthquake              0.995543   0.994426  0.958624  0.976197\n",
      "cold                    0.996414   0.996875  0.822165  0.901130\n",
      "other_weather           0.989652   0.997570  0.804114  0.890456\n",
      "direct_report           0.981353   0.998256  0.905616  0.949682\n"
     ]
    }
   ],
   "source": [
    "# Calculate evaluation metrics for training set\n",
    "y_train_pred = pipeline.predict(X_train)\n",
    "col_names = list(y.columns.values)\n",
    "\n",
    "print(eval_metrics(np.array(y_train), y_train_pred, col_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Accuracy  Precision    Recall        F1\n",
      "related                 0.818042   0.851322  0.925379  0.886807\n",
      "request                 0.882434   0.781701  0.436380  0.560092\n",
      "offer                   0.995390   0.000000  0.000000  0.000000\n",
      "aid_related             0.755494   0.742035  0.624581  0.678261\n",
      "medical_help            0.920240   0.637681  0.081784  0.144975\n",
      "medical_products        0.950515   0.600000  0.081571  0.143617\n",
      "search_and_rescue       0.973260   0.583333  0.078652  0.138614\n",
      "security                0.982173   0.000000  0.000000  0.000000\n",
      "military                0.969110   0.523810  0.054455  0.098655\n",
      "water                   0.956047   0.835106  0.381068  0.523333\n",
      "food                    0.932995   0.816017  0.517857  0.633613\n",
      "shelter                 0.929307   0.792627  0.293015  0.427861\n",
      "clothing                0.984939   0.681818  0.141509  0.234375\n",
      "money                   0.978177   0.666667  0.027778  0.053333\n",
      "missing_people          0.989550   0.333333  0.014925  0.028571\n",
      "refugees                0.966805   0.631579  0.054299  0.100000\n",
      "death                   0.962041   0.771084  0.219178  0.341333\n",
      "other_aid               0.865683   0.426667  0.037080  0.068230\n",
      "infrastructure_related  0.935915   0.333333  0.007246  0.014184\n",
      "transport               0.954049   0.560000  0.046358  0.085627\n",
      "buildings               0.952052   0.730769  0.113095  0.195876\n",
      "electricity             0.978946   0.666667  0.028777  0.055172\n",
      "tools                   0.995390   0.000000  0.000000  0.000000\n",
      "hospitals               0.989089   0.000000  0.000000  0.000000\n",
      "shops                   0.996465   0.000000  0.000000  0.000000\n",
      "aid_centers             0.988474   0.000000  0.000000  0.000000\n",
      "other_infrastructure    0.955586   0.000000  0.000000  0.000000\n",
      "weather_related         0.868449   0.844303  0.652269  0.735965\n",
      "floods                  0.942370   0.896714  0.351103  0.504624\n",
      "storm                   0.936530   0.743719  0.487644  0.589055\n",
      "fire                    0.988628   0.714286  0.064935  0.119048\n",
      "earthquake              0.961734   0.866242  0.686869  0.766197\n",
      "cold                    0.979560   0.736842  0.098592  0.173913\n",
      "other_weather           0.944829   0.416667  0.028169  0.052770\n",
      "direct_report           0.842631   0.710098  0.340094  0.459916\n"
     ]
    }
   ],
   "source": [
    "# Calculate evaluation metrics for test set\n",
    "y_test_pred = pipeline.predict(X_test)\n",
    "\n",
    "eval_metrics0 = eval_metrics(np.array(y_test), y_test_pred, col_names)\n",
    "print(eval_metrics0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=<function tokenize at 0x7f367844a620>, vocabulary=None)),\n",
       "  ('tfidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       "  ('clf',\n",
       "   MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "               max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "               oob_score=False, random_state=None, verbose=0,\n",
       "               warm_start=False),\n",
       "              n_jobs=1))],\n",
       " 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=<function tokenize at 0x7f367844a620>, vocabulary=None),\n",
       " 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'clf': MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "             oob_score=False, random_state=None, verbose=0,\n",
       "             warm_start=False),\n",
       "            n_jobs=1),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': <function __main__.tokenize(text)>,\n",
       " 'vect__vocabulary': None,\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': True,\n",
       " 'clf__estimator__bootstrap': True,\n",
       " 'clf__estimator__class_weight': None,\n",
       " 'clf__estimator__criterion': 'gini',\n",
       " 'clf__estimator__max_depth': None,\n",
       " 'clf__estimator__max_features': 'auto',\n",
       " 'clf__estimator__max_leaf_nodes': None,\n",
       " 'clf__estimator__min_impurity_decrease': 0.0,\n",
       " 'clf__estimator__min_impurity_split': None,\n",
       " 'clf__estimator__min_samples_leaf': 1,\n",
       " 'clf__estimator__min_samples_split': 2,\n",
       " 'clf__estimator__min_weight_fraction_leaf': 0.0,\n",
       " 'clf__estimator__n_estimators': 10,\n",
       " 'clf__estimator__n_jobs': 1,\n",
       " 'clf__estimator__oob_score': False,\n",
       " 'clf__estimator__random_state': None,\n",
       " 'clf__estimator__verbose': 0,\n",
       " 'clf__estimator__warm_start': False,\n",
       " 'clf__estimator': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "             oob_score=False, random_state=None, verbose=0,\n",
       "             warm_start=False),\n",
       " 'clf__n_jobs': 1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Review params to get ideas to improve model\n",
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 8 candidates, totalling 16 fits\n",
      "[CV] clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.2338899702899293, total= 1.8min\n",
      "[CV] clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.22961065573770492, total= 1.8min\n",
      "[CV] clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  5.0min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.24024177850630057, total= 2.2min\n",
      "[CV] clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.23278688524590163, total= 2.2min\n",
      "[CV] clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.23552914660383156, total= 1.8min\n",
      "[CV] clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.22899590163934427, total= 1.8min\n",
      "[CV] clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.23911484479049278, total= 2.3min\n",
      "[CV] clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.23237704918032787, total= 2.2min\n",
      "[CV] clf__estimator__n_estimators=20, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=20, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.24782296895809855, total= 2.2min\n",
      "[CV] clf__estimator__n_estimators=20, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=20, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.23698770491803278, total= 2.2min\n",
      "[CV] clf__estimator__n_estimators=20, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=20, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.25458457125294537, total= 3.1min\n",
      "[CV] clf__estimator__n_estimators=20, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=20, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.24764344262295082, total= 2.9min\n",
      "[CV] clf__estimator__n_estimators=20, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=20, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.2448519618891507, total= 2.2min\n",
      "[CV] clf__estimator__n_estimators=20, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=20, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.2355532786885246, total= 2.2min\n",
      "[CV] clf__estimator__n_estimators=20, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=20, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.24331523409486733, total= 3.1min\n",
      "[CV] clf__estimator__n_estimators=20, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=20, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.23452868852459016, total= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  16 out of  16 | elapsed: 49.9min finished\n"
     ]
    }
   ],
   "source": [
    "# Perform grid search to find better parameters\n",
    "parameters = {'tfidf__use_idf': (True, False),\n",
    "              'vect__ngram_range': ((1, 1), (1, 2)),\n",
    "              'clf__estimator__n_estimators': [10,20],\n",
    "}\n",
    "cv = GridSearchCV(estimator=pipeline, param_grid=parameters, cv=2, n_jobs=-1, verbose=3)\n",
    "\n",
    "# Find best parameters\n",
    "T_model = cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([  63.16922081,   86.16967964,   63.80121577,   88.35545719,\n",
       "          84.76486337,  131.52965391,   86.42719221,  139.10420871]),\n",
       " 'std_fit_time': array([ 1.23045409,  2.87658978,  1.4163779 ,  3.41441476,  2.00575626,\n",
       "         5.84891903,  2.03989244,  1.78708339]),\n",
       " 'mean_score_time': array([ 43.9661237 ,  45.47683251,  44.20432281,  45.65678787,\n",
       "         45.97858405,  48.80975175,  46.11071301,  54.59068859]),\n",
       " 'std_score_time': array([ 0.75211608,  0.82379854,  1.07510853,  0.7555356 ,  0.73018622,\n",
       "         0.53480983,  0.72210574,  6.48409736]),\n",
       " 'param_clf__estimator__n_estimators': masked_array(data = [10 10 10 10 20 20 20 20],\n",
       "              mask = [False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_tfidf__use_idf': masked_array(data = [True True False False True True False False],\n",
       "              mask = [False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_vect__ngram_range': masked_array(data = [(1, 1) (1, 2) (1, 1) (1, 2) (1, 1) (1, 2) (1, 1) (1, 2)],\n",
       "              mask = [False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'params': [{'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__ngram_range': (1, 1)},\n",
       "  {'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__ngram_range': (1, 2)},\n",
       "  {'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__ngram_range': (1, 1)},\n",
       "  {'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__ngram_range': (1, 2)},\n",
       "  {'clf__estimator__n_estimators': 20,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__ngram_range': (1, 1)},\n",
       "  {'clf__estimator__n_estimators': 20,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__ngram_range': (1, 2)},\n",
       "  {'clf__estimator__n_estimators': 20,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__ngram_range': (1, 1)},\n",
       "  {'clf__estimator__n_estimators': 20,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__ngram_range': (1, 2)}],\n",
       " 'split0_test_score': array([ 0.23388997,  0.24024178,  0.23552915,  0.23911484,  0.24782297,\n",
       "         0.25458457,  0.24485196,  0.24331523]),\n",
       " 'split1_test_score': array([ 0.22961066,  0.23278689,  0.2289959 ,  0.23237705,  0.2369877 ,\n",
       "         0.24764344,  0.23555328,  0.23452869]),\n",
       " 'mean_test_score': array([ 0.23175042,  0.23651452,  0.23226269,  0.23574612,  0.24240561,\n",
       "         0.25111418,  0.24020286,  0.23892219]),\n",
       " 'std_test_score': array([ 0.00213966,  0.00372745,  0.00326662,  0.0033689 ,  0.00541763,\n",
       "         0.00347056,  0.00464934,  0.00439327]),\n",
       " 'rank_test_score': array([8, 5, 7, 6, 2, 1, 3, 4], dtype=int32),\n",
       " 'split0_train_score': array([ 0.81260246,  0.77704918,  0.81004098,  0.78739754,  0.90901639,\n",
       "         0.89129098,  0.9079918 ,  0.89262295]),\n",
       " 'split1_train_score': array([ 0.80606495,  0.78506301,  0.8095482 ,  0.7854728 ,  0.90964041,\n",
       "         0.89703924,  0.91517263,  0.896527  ]),\n",
       " 'mean_train_score': array([ 0.80933371,  0.78105609,  0.80979459,  0.78643517,  0.9093284 ,\n",
       "         0.89416511,  0.91158221,  0.89457497]),\n",
       " 'std_train_score': array([ 0.00326875,  0.00400691,  0.00024639,  0.00096237,  0.00031201,\n",
       "         0.00287413,  0.00359041,  0.00195202])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grid search results\n",
    "T_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25111418472414321"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top mean score\n",
    "np.max(T_model.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__estimator__n_estimators': 20,\n",
       " 'tfidf__use_idf': True,\n",
       " 'vect__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters for top mean score\n",
    "T_model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Accuracy  Precision    Recall        F1\n",
      "related                 0.814200   0.853242  0.916401  0.883694\n",
      "request                 0.884278   0.766520  0.467742  0.580968\n",
      "offer                   0.995390   0.000000  0.000000  0.000000\n",
      "aid_related             0.757799   0.751246  0.617505  0.677841\n",
      "medical_help            0.918857   0.578125  0.068773  0.122924\n",
      "medical_products        0.953127   0.782609  0.108761  0.190981\n",
      "search_and_rescue       0.973413   0.647059  0.061798  0.112821\n",
      "security                0.982173   0.000000  0.000000  0.000000\n",
      "military                0.968495   0.428571  0.044554  0.080717\n",
      "water                   0.959736   0.867647  0.429612  0.574675\n",
      "food                    0.937452   0.842217  0.542582  0.659983\n",
      "shelter                 0.929153   0.800000  0.286201  0.421581\n",
      "clothing                0.984017   0.600000  0.056604  0.103448\n",
      "money                   0.978331   0.800000  0.027778  0.053691\n",
      "missing_people          0.988935   0.000000  0.000000  0.000000\n",
      "refugees                0.966190   0.545455  0.027149  0.051724\n",
      "death                   0.961580   0.862069  0.171233  0.285714\n",
      "other_aid               0.868757   0.567164  0.044032  0.081720\n",
      "infrastructure_related  0.936069   0.250000  0.002415  0.004785\n",
      "transport               0.954818   0.653846  0.056291  0.103659\n",
      "buildings               0.952666   0.769231  0.119048  0.206186\n",
      "electricity             0.978638   0.500000  0.007194  0.014184\n",
      "tools                   0.995390   0.000000  0.000000  0.000000\n",
      "hospitals               0.989089   0.000000  0.000000  0.000000\n",
      "shops                   0.996465   0.000000  0.000000  0.000000\n",
      "aid_centers             0.988628   0.000000  0.000000  0.000000\n",
      "other_infrastructure    0.956355   0.000000  0.000000  0.000000\n",
      "weather_related         0.859690   0.859498  0.598688  0.705769\n",
      "floods                  0.941294   0.871560  0.349265  0.498688\n",
      "storm                   0.933149   0.777419  0.397035  0.525627\n",
      "fire                    0.988167   0.500000  0.012987  0.025316\n",
      "earthquake              0.967112   0.880000  0.740741  0.804388\n",
      "cold                    0.979253   1.000000  0.049296  0.093960\n",
      "other_weather           0.945904   0.615385  0.022535  0.043478\n",
      "direct_report           0.846166   0.740995  0.336973  0.463271\n"
     ]
    }
   ],
   "source": [
    "# Evaluate metrics for test set\n",
    "tuned_pred_test = T_model.predict(X_test)\n",
    "\n",
    "eval_metrics1 = eval_metrics(np.array(y_test), tuned_pred_test, col_names)\n",
    "\n",
    "print(eval_metrics1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.943511</td>\n",
       "      <td>0.539841</td>\n",
       "      <td>0.196419</td>\n",
       "      <td>0.251829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.055402</td>\n",
       "      <td>0.307257</td>\n",
       "      <td>0.247707</td>\n",
       "      <td>0.271017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.755494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.934455</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.021352</td>\n",
       "      <td>0.040671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.956047</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.078652</td>\n",
       "      <td>0.138614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.980867</td>\n",
       "      <td>0.757401</td>\n",
       "      <td>0.345598</td>\n",
       "      <td>0.482270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.996465</td>\n",
       "      <td>0.896714</td>\n",
       "      <td>0.925379</td>\n",
       "      <td>0.886807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Accuracy  Precision     Recall         F1\n",
       "count  35.000000  35.000000  35.000000  35.000000\n",
       "mean    0.943511   0.539841   0.196419   0.251829\n",
       "std     0.055402   0.307257   0.247707   0.271017\n",
       "min     0.755494   0.000000   0.000000   0.000000\n",
       "25%     0.934455   0.375000   0.021352   0.040671\n",
       "50%     0.956047   0.666667   0.078652   0.138614\n",
       "75%     0.980867   0.757401   0.345598   0.482270\n",
       "max     0.996465   0.896714   0.925379   0.886807"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary of first model\n",
    "eval_metrics0.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.943735</td>\n",
       "      <td>0.545996</td>\n",
       "      <td>0.187520</td>\n",
       "      <td>0.239194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.055492</td>\n",
       "      <td>0.336340</td>\n",
       "      <td>0.251418</td>\n",
       "      <td>0.278861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.757799</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.934609</td>\n",
       "      <td>0.339286</td>\n",
       "      <td>0.004805</td>\n",
       "      <td>0.009485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.959736</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.056291</td>\n",
       "      <td>0.103448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.980713</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.343119</td>\n",
       "      <td>0.480979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.996465</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.916401</td>\n",
       "      <td>0.883694</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Accuracy  Precision     Recall         F1\n",
       "count  35.000000  35.000000  35.000000  35.000000\n",
       "mean    0.943735   0.545996   0.187520   0.239194\n",
       "std     0.055492   0.336340   0.251418   0.278861\n",
       "min     0.757799   0.000000   0.000000   0.000000\n",
       "25%     0.934609   0.339286   0.004805   0.009485\n",
       "50%     0.959736   0.647059   0.056291   0.103448\n",
       "75%     0.980713   0.800000   0.343119   0.480979\n",
       "max     0.996465   1.000000   0.916401   0.883694"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary of tuned model\n",
    "eval_metrics1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improve the pipeline\n",
    "pipeline2 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('best', TruncatedSVD()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(AdaBoostClassifier()))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...mator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None),\n",
       "           n_jobs=1))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# Train pipeline 2\n",
    "pipeline2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Accuracy  Precision    Recall        F1\n",
      "related                 0.765842   0.765967  0.999398  0.867249\n",
      "request                 0.828390   1.000000  0.000895  0.001788\n",
      "offer                   0.995185   1.000000  0.010526  0.020833\n",
      "aid_related             0.586394   0.518904  0.116296  0.190008\n",
      "medical_help            0.920240   1.000000  0.001283  0.002562\n",
      "medical_products        0.949490   1.000000  0.002024  0.004040\n",
      "search_and_rescue       0.971825   1.000000  0.007220  0.014337\n",
      "security                0.982275   1.000000  0.002882  0.005747\n",
      "military                0.966856   1.000000  0.003082  0.006144\n",
      "water                   0.936069   1.000000  0.001600  0.003195\n",
      "food                    0.888069   1.000000  0.000457  0.000914\n",
      "shelter                 0.911377   1.000000  0.002882  0.005747\n",
      "clothing                0.984888   1.000000  0.006734  0.013378\n",
      "money                   0.976538   1.000000  0.004348  0.008658\n",
      "missing_people          0.988371   1.000000  0.008734  0.017316\n",
      "refugees                0.966600   1.000000  0.006098  0.012121\n",
      "death                   0.953998   1.000000  0.002222  0.004435\n",
      "other_aid               0.867066   0.800000  0.001540  0.003073\n",
      "infrastructure_related  0.935505   1.000000  0.001586  0.003167\n",
      "transport               0.953742   1.000000  0.003311  0.006601\n",
      "buildings               0.949132   1.000000  0.002010  0.004012\n",
      "electricity             0.980022   1.000000  0.005102  0.010152\n",
      "tools                   0.994058   1.000000  0.016949  0.033333\n",
      "hospitals               0.989908   1.000000  0.010050  0.019900\n",
      "shops                   0.995799   1.000000  0.023810  0.046512\n",
      "aid_centers             0.988064   1.000000  0.008511  0.016878\n",
      "other_infrastructure    0.956560   1.000000  0.002353  0.004695\n",
      "weather_related         0.719840   0.653061  0.005835  0.011567\n",
      "floods                  0.918139   1.000000  0.001874  0.003741\n",
      "storm                   0.906204   1.000000  0.000546  0.001091\n",
      "fire                    0.989294   1.000000  0.009479  0.018779\n",
      "earthquake              0.903591   1.000000  0.000531  0.001062\n",
      "cold                    0.979202   1.000000  0.002457  0.004902\n",
      "other_weather           0.947236   1.000000  0.000970  0.001938\n",
      "direct_report           0.806362   1.000000  0.002112  0.004215\n"
     ]
    }
   ],
   "source": [
    "# Calculate evaluation metrics for training set 2\n",
    "y_train_pred2 = pipeline2.predict(X_train)\n",
    "col_names_ = list(y.columns.values)\n",
    "\n",
    "print(eval_metrics(np.array(y_train), y_train_pred2, col_names_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Accuracy  Precision    Recall        F1\n",
      "related                 0.764100   0.764289  0.998993  0.866021\n",
      "request                 0.827724   0.000000  0.000000  0.000000\n",
      "offer                   0.996465   0.000000  0.000000  0.000000\n",
      "aid_related             0.581374   0.494436  0.114464  0.185894\n",
      "medical_help            0.919318   0.000000  0.000000  0.000000\n",
      "medical_products        0.950054   0.000000  0.000000  0.000000\n",
      "search_and_rescue       0.973874   0.000000  0.000000  0.000000\n",
      "security                0.980944   0.000000  0.000000  0.000000\n",
      "military                0.967420   0.000000  0.000000  0.000000\n",
      "water                   0.935147   0.000000  0.000000  0.000000\n",
      "food                    0.886737   0.000000  0.000000  0.000000\n",
      "shelter                 0.910250   0.000000  0.000000  0.000000\n",
      "clothing                0.983249   0.000000  0.000000  0.000000\n",
      "money                   0.977716   0.000000  0.000000  0.000000\n",
      "missing_people          0.989089   0.000000  0.000000  0.000000\n",
      "refugees                0.966344   0.000000  0.000000  0.000000\n",
      "death                   0.954664   0.000000  0.000000  0.000000\n",
      "other_aid               0.869832   1.000000  0.001179  0.002356\n",
      "infrastructure_related  0.931612   0.000000  0.000000  0.000000\n",
      "transport               0.954357   0.000000  0.000000  0.000000\n",
      "buildings               0.947749   0.000000  0.000000  0.000000\n",
      "electricity             0.978485   0.000000  0.000000  0.000000\n",
      "tools                   0.993699   0.000000  0.000000  0.000000\n",
      "hospitals               0.986937   0.000000  0.000000  0.000000\n",
      "shops                   0.994314   0.000000  0.000000  0.000000\n",
      "aid_centers             0.988628   0.000000  0.000000  0.000000\n",
      "other_infrastructure    0.953588   0.000000  0.000000  0.000000\n",
      "weather_related         0.721992   0.600000  0.006619  0.013093\n",
      "floods                  0.914861   0.000000  0.000000  0.000000\n",
      "storm                   0.906101   0.000000  0.000000  0.000000\n",
      "fire                    0.989089   0.000000  0.000000  0.000000\n",
      "earthquake              0.912095   0.000000  0.000000  0.000000\n",
      "cold                    0.981097   0.000000  0.000000  0.000000\n",
      "other_weather           0.946980   0.000000  0.000000  0.000000\n",
      "direct_report           0.802059   0.333333  0.000777  0.001550\n"
     ]
    }
   ],
   "source": [
    "# Calculate evaluation metrics for test set 2\n",
    "y_test_pred2 = pipeline2.predict(X_test)\n",
    "\n",
    "eval_metrics2 = eval_metrics(np.array(y_test), y_test_pred2, col_names)\n",
    "print(eval_metrics2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None)),\n",
       "  ('best', TruncatedSVD(algorithm='randomized', n_components=2, n_iter=5,\n",
       "          random_state=None, tol=0.0)),\n",
       "  ('tfidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       "  ('clf',\n",
       "   MultiOutputClassifier(estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "             learning_rate=1.0, n_estimators=50, random_state=None),\n",
       "              n_jobs=1))],\n",
       " 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       " 'best': TruncatedSVD(algorithm='randomized', n_components=2, n_iter=5,\n",
       "        random_state=None, tol=0.0),\n",
       " 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'clf': MultiOutputClassifier(estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "           learning_rate=1.0, n_estimators=50, random_state=None),\n",
       "            n_jobs=1),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': None,\n",
       " 'vect__vocabulary': None,\n",
       " 'best__algorithm': 'randomized',\n",
       " 'best__n_components': 2,\n",
       " 'best__n_iter': 5,\n",
       " 'best__random_state': None,\n",
       " 'best__tol': 0.0,\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': True,\n",
       " 'clf__estimator__algorithm': 'SAMME.R',\n",
       " 'clf__estimator__base_estimator': None,\n",
       " 'clf__estimator__learning_rate': 1.0,\n",
       " 'clf__estimator__n_estimators': 50,\n",
       " 'clf__estimator__random_state': None,\n",
       " 'clf__estimator': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "           learning_rate=1.0, n_estimators=50, random_state=None),\n",
       " 'clf__n_jobs': 1}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at parameters for improving model\n",
    "pipeline2.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 8 candidates, totalling 16 fits\n",
      "[CV] clf__estimator__learning_rate=1, clf__estimator__n_estimators=50, tfidf__use_idf=True \n",
      "[CV]  clf__estimator__learning_rate=1, clf__estimator__n_estimators=50, tfidf__use_idf=True, score=0.17723593894068232, total=  20.2s\n",
      "[CV] clf__estimator__learning_rate=1, clf__estimator__n_estimators=50, tfidf__use_idf=True \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   23.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__learning_rate=1, clf__estimator__n_estimators=50, tfidf__use_idf=True, score=0.19088114754098362, total=  20.3s\n",
      "[CV] clf__estimator__learning_rate=1, clf__estimator__n_estimators=50, tfidf__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   46.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__learning_rate=1, clf__estimator__n_estimators=50, tfidf__use_idf=False, score=0.17682614486220674, total=  20.2s\n",
      "[CV] clf__estimator__learning_rate=1, clf__estimator__n_estimators=50, tfidf__use_idf=False \n",
      "[CV]  clf__estimator__learning_rate=1, clf__estimator__n_estimators=50, tfidf__use_idf=False, score=0.18770491803278688, total=  20.3s\n",
      "[CV] clf__estimator__learning_rate=1, clf__estimator__n_estimators=100, tfidf__use_idf=True \n",
      "[CV]  clf__estimator__learning_rate=1, clf__estimator__n_estimators=100, tfidf__use_idf=True, score=0.16678618993955538, total=  39.8s\n",
      "[CV] clf__estimator__learning_rate=1, clf__estimator__n_estimators=100, tfidf__use_idf=True \n",
      "[CV]  clf__estimator__learning_rate=1, clf__estimator__n_estimators=100, tfidf__use_idf=True, score=0.18575819672131147, total=  39.5s\n",
      "[CV] clf__estimator__learning_rate=1, clf__estimator__n_estimators=100, tfidf__use_idf=False \n",
      "[CV]  clf__estimator__learning_rate=1, clf__estimator__n_estimators=100, tfidf__use_idf=False, score=0.17805552709763345, total=  39.4s\n",
      "[CV] clf__estimator__learning_rate=1, clf__estimator__n_estimators=100, tfidf__use_idf=False \n",
      "[CV]  clf__estimator__learning_rate=1, clf__estimator__n_estimators=100, tfidf__use_idf=False, score=0.1880122950819672, total=  39.5s\n",
      "[CV] clf__estimator__learning_rate=2, clf__estimator__n_estimators=50, tfidf__use_idf=True \n",
      "[CV]  clf__estimator__learning_rate=2, clf__estimator__n_estimators=50, tfidf__use_idf=True, score=0.0, total=  28.6s\n",
      "[CV] clf__estimator__learning_rate=2, clf__estimator__n_estimators=50, tfidf__use_idf=True \n",
      "[CV]  clf__estimator__learning_rate=2, clf__estimator__n_estimators=50, tfidf__use_idf=True, score=0.0, total=  28.2s\n",
      "[CV] clf__estimator__learning_rate=2, clf__estimator__n_estimators=50, tfidf__use_idf=False \n",
      "[CV]  clf__estimator__learning_rate=2, clf__estimator__n_estimators=50, tfidf__use_idf=False, score=0.0, total=  29.3s\n",
      "[CV] clf__estimator__learning_rate=2, clf__estimator__n_estimators=50, tfidf__use_idf=False \n",
      "[CV]  clf__estimator__learning_rate=2, clf__estimator__n_estimators=50, tfidf__use_idf=False, score=0.0, total=  28.3s\n",
      "[CV] clf__estimator__learning_rate=2, clf__estimator__n_estimators=100, tfidf__use_idf=True \n",
      "[CV]  clf__estimator__learning_rate=2, clf__estimator__n_estimators=100, tfidf__use_idf=True, score=0.0, total= 1.1min\n",
      "[CV] clf__estimator__learning_rate=2, clf__estimator__n_estimators=100, tfidf__use_idf=True \n",
      "[CV]  clf__estimator__learning_rate=2, clf__estimator__n_estimators=100, tfidf__use_idf=True, score=0.0, total= 1.1min\n",
      "[CV] clf__estimator__learning_rate=2, clf__estimator__n_estimators=100, tfidf__use_idf=False \n",
      "[CV]  clf__estimator__learning_rate=2, clf__estimator__n_estimators=100, tfidf__use_idf=False, score=0.0, total= 1.2min\n",
      "[CV] clf__estimator__learning_rate=2, clf__estimator__n_estimators=100, tfidf__use_idf=False \n",
      "[CV]  clf__estimator__learning_rate=2, clf__estimator__n_estimators=100, tfidf__use_idf=False, score=0.0, total= 1.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  16 out of  16 | elapsed: 12.7min finished\n"
     ]
    }
   ],
   "source": [
    "# Model tuning with added parameters\n",
    "parameters2 = {'tfidf__use_idf': (True, False), \n",
    "               'clf__estimator__n_estimators': [50, 100],\n",
    "               'clf__estimator__learning_rate': [1,2]\n",
    "              }\n",
    "cv2 = GridSearchCV(estimator=pipeline2, param_grid=parameters2, cv=2, n_jobs=-1, verbose=3)\n",
    "\n",
    "# Find best parameters\n",
    "T_model2 = cv2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18405819374007482"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top mean score for model 2\n",
    "np.max(T_model2.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__estimator__learning_rate': 1,\n",
       " 'clf__estimator__n_estimators': 50,\n",
       " 'tfidf__use_idf': True}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters for top mean score model 2\n",
    "T_model2.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Accuracy  Precision    Recall        F1\n",
      "related                 0.764100   0.764289  0.998993  0.866021\n",
      "request                 0.827724   0.000000  0.000000  0.000000\n",
      "offer                   0.996465   0.000000  0.000000  0.000000\n",
      "aid_related             0.581374   0.494436  0.114464  0.185894\n",
      "medical_help            0.919318   0.000000  0.000000  0.000000\n",
      "medical_products        0.950054   0.000000  0.000000  0.000000\n",
      "search_and_rescue       0.973874   0.000000  0.000000  0.000000\n",
      "security                0.980944   0.000000  0.000000  0.000000\n",
      "military                0.967266   0.000000  0.000000  0.000000\n",
      "water                   0.935147   0.000000  0.000000  0.000000\n",
      "food                    0.886737   0.000000  0.000000  0.000000\n",
      "shelter                 0.910250   0.000000  0.000000  0.000000\n",
      "clothing                0.983249   0.000000  0.000000  0.000000\n",
      "money                   0.977716   0.000000  0.000000  0.000000\n",
      "missing_people          0.989089   0.000000  0.000000  0.000000\n",
      "refugees                0.966344   0.000000  0.000000  0.000000\n",
      "death                   0.954664   0.000000  0.000000  0.000000\n",
      "other_aid               0.869832   1.000000  0.001179  0.002356\n",
      "infrastructure_related  0.931612   0.000000  0.000000  0.000000\n",
      "transport               0.954357   0.000000  0.000000  0.000000\n",
      "buildings               0.947749   0.000000  0.000000  0.000000\n",
      "electricity             0.978485   0.000000  0.000000  0.000000\n",
      "tools                   0.993699   0.000000  0.000000  0.000000\n",
      "hospitals               0.986937   0.000000  0.000000  0.000000\n",
      "shops                   0.994467   0.000000  0.000000  0.000000\n",
      "aid_centers             0.988628   0.000000  0.000000  0.000000\n",
      "other_infrastructure    0.953588   0.000000  0.000000  0.000000\n",
      "weather_related         0.721992   0.600000  0.006619  0.013093\n",
      "floods                  0.914861   0.000000  0.000000  0.000000\n",
      "storm                   0.905640   0.000000  0.000000  0.000000\n",
      "fire                    0.989089   0.000000  0.000000  0.000000\n",
      "earthquake              0.911787   0.375000  0.005245  0.010345\n",
      "cold                    0.981097   0.000000  0.000000  0.000000\n",
      "other_weather           0.946980   0.000000  0.000000  0.000000\n",
      "direct_report           0.802059   0.333333  0.000777  0.001550\n"
     ]
    }
   ],
   "source": [
    "# Evaluating metrics for test set model 2\n",
    "tuned_pred_test2 = T_model2.predict(X_test)\n",
    "\n",
    "eval_metrics2 = eval_metrics(np.array(y_test), tuned_pred_test2, col_names)\n",
    "\n",
    "print(eval_metrics2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.943511</td>\n",
       "      <td>0.539841</td>\n",
       "      <td>0.196419</td>\n",
       "      <td>0.251829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.055402</td>\n",
       "      <td>0.307257</td>\n",
       "      <td>0.247707</td>\n",
       "      <td>0.271017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.755494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.934455</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.021352</td>\n",
       "      <td>0.040671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.956047</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.078652</td>\n",
       "      <td>0.138614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.980867</td>\n",
       "      <td>0.757401</td>\n",
       "      <td>0.345598</td>\n",
       "      <td>0.482270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.996465</td>\n",
       "      <td>0.896714</td>\n",
       "      <td>0.925379</td>\n",
       "      <td>0.886807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Accuracy  Precision     Recall         F1\n",
       "count  35.000000  35.000000  35.000000  35.000000\n",
       "mean    0.943511   0.539841   0.196419   0.251829\n",
       "std     0.055402   0.307257   0.247707   0.271017\n",
       "min     0.755494   0.000000   0.000000   0.000000\n",
       "25%     0.934455   0.375000   0.021352   0.040671\n",
       "50%     0.956047   0.666667   0.078652   0.138614\n",
       "75%     0.980867   0.757401   0.345598   0.482270\n",
       "max     0.996465   0.896714   0.925379   0.886807"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary of original model\n",
    "eval_metrics0.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.943735</td>\n",
       "      <td>0.545996</td>\n",
       "      <td>0.187520</td>\n",
       "      <td>0.239194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.055492</td>\n",
       "      <td>0.336340</td>\n",
       "      <td>0.251418</td>\n",
       "      <td>0.278861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.757799</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.934609</td>\n",
       "      <td>0.339286</td>\n",
       "      <td>0.004805</td>\n",
       "      <td>0.009485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.959736</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.056291</td>\n",
       "      <td>0.103448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.980713</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.343119</td>\n",
       "      <td>0.480979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.996465</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.916401</td>\n",
       "      <td>0.883694</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Accuracy  Precision     Recall         F1\n",
       "count  35.000000  35.000000  35.000000  35.000000\n",
       "mean    0.943735   0.545996   0.187520   0.239194\n",
       "std     0.055492   0.336340   0.251418   0.278861\n",
       "min     0.757799   0.000000   0.000000   0.000000\n",
       "25%     0.934609   0.339286   0.004805   0.009485\n",
       "50%     0.959736   0.647059   0.056291   0.103448\n",
       "75%     0.980713   0.800000   0.343119   0.480979\n",
       "max     0.996465   1.000000   0.916401   0.883694"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary of tuned model 1\n",
    "eval_metrics1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.923919</td>\n",
       "      <td>0.101916</td>\n",
       "      <td>0.032208</td>\n",
       "      <td>0.030836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.089345</td>\n",
       "      <td>0.247148</td>\n",
       "      <td>0.169329</td>\n",
       "      <td>0.148676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.581374</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.911019</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.953588</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.981020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.996465</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998993</td>\n",
       "      <td>0.866021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Accuracy  Precision     Recall         F1\n",
       "count  35.000000  35.000000  35.000000  35.000000\n",
       "mean    0.923919   0.101916   0.032208   0.030836\n",
       "std     0.089345   0.247148   0.169329   0.148676\n",
       "min     0.581374   0.000000   0.000000   0.000000\n",
       "25%     0.911019   0.000000   0.000000   0.000000\n",
       "50%     0.953588   0.000000   0.000000   0.000000\n",
       "75%     0.981020   0.000000   0.000000   0.000000\n",
       "max     0.996465   1.000000   0.998993   0.866021"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary of tuned model 2\n",
    "eval_metrics2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Tuned Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Model 1 has higher mean in all categories (accuracy, precision, recall, F1 value). The standard deviation for model 1 is higher, indicating a greater distribution in relation to the mean values\n",
    "* Model 2 has lower mean value for all categories and smaller standard deviation for each metric when compared to model 1\n",
    "\n",
    "* Comparing metrics (mean value) for model 1 and model 2\n",
    ">* **Accuracy**: Both model 1 and model 2 have pretty high accuracy (0.942185 and 0.925368, respectively)\n",
    ">* **Precision**: Model 1 has a higher average precision of 0.559671 when compared to model 2 (0.081468). The higher precision of model 1, indicates that this model is better at predicting actual positives and is better for predicting when the costs of a false positive is high.\n",
    ">* **Recall**: Model 1 has a higer recall of 0.171154 when compared to model 2's recall of 0.030304. The higher recall value in model 1, calculates how many of the actual positives the model captures that are a true positive. This means model 1 would be best selected when there's a high cost associated with false negatives. \n",
    ">* **F1**: Model 1 has a higher F1 value of 0.225254 when compared to model 2 (0.028043). The F1 score is best used if we're seeking to find a balance between Precision and Recall and if there's an uneven distribution. An F1 score of 1 is considered a 'perfect' model, while and F1 score of 0 indicates a bad model. The higher F1 value for model 1 is indicative that the model has low false positives and low false negatives, so it's more effective at correctly identifying real threats and isn't disturbed by false alarms. \n",
    "\n",
    "* Based on the metrics, Model 1 is the best overall model and will be saved for continuation of this work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export best model as pickle file\n",
    "pickle.dump(T_model, open('classifer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources\n",
    "* https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9\n",
    "* https://pathmind.com/wiki/accuracy-precision-recall-f1#:~:text=That%20is%2C%20a%20good%20F1,total%20failure%20when%20it's%200%20\n",
    "* https://apapiu.github.io/2016-08-04-tf_idf/\n",
    "* https://www.greenbook.org/marketing-research/how-to-interpret-standard-deviation-and-standard-error-in-survey-research-03377\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
