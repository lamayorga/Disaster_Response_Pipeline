{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import nltk\n",
    "nltk.download(['punkt', 'wordnet', 'averaged_perceptron_tagger', 'stopwords'])\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from database\n",
    "engine = create_engine('sqlite:///Messages.db')\n",
    "df = pd.read_sql_table('Messages',engine)\n",
    "X = df.message.values\n",
    "y = df.iloc[:,4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write tokenization function to process text data\n",
    "def tokenize(text):\n",
    "    \n",
    "    # Normalize text\n",
    "    text = re.sub(r'[^a-zA-Z0-9]',' ',text.lower())\n",
    "\n",
    "    # Token messages\n",
    "    words = word_tokenize(text)\n",
    "    tokens = [w for w in words if w not in stopwords.words(\"english\")]\n",
    "\n",
    "    # Lemmatizer and clean\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        clean_tok = lemmatizer.lemmatize(tok).strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build machine learning pipeline\n",
    "pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# Train pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate metrics of ML pipeline \n",
    "def eval_metrics(ArrayL, ArrayP, col_names):\n",
    "    \n",
    "    \"\"\"Evalute metrics of the ML pipeline model\n",
    "    \n",
    "    inputs:\n",
    "    ArrayL: array. Array containing the real labels.\n",
    "    ArrayP: array. Array containing predicted labels.\n",
    "    col_names: list of strings. List containing names for each of the ArrayP fields.\n",
    "       \n",
    "    Returns:\n",
    "    data_metrics: Contains accuracy, precision, recall \n",
    "    and f1 score for a given set of ArrayL and ArrayP labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    metrics = []\n",
    "    \n",
    "    # Evaluate metrics for each set of labels\n",
    "    for i in range(len(col_names)):\n",
    "        accuracy = accuracy_score(ArrayL[:, i], ArrayP[:, i])\n",
    "        precision = precision_score(ArrayL[:, i], ArrayP[:, i])\n",
    "        recall = recall_score(ArrayL[:, i], ArrayP[:, i])\n",
    "        f1 = f1_score(ArrayL[:, i], ArrayP[:, i])\n",
    "        \n",
    "        metrics.append([accuracy, precision, recall, f1])\n",
    "    \n",
    "    # Store metrics\n",
    "    metrics = np.array(metrics)\n",
    "    data_metrics = pd.DataFrame(data = metrics, index = col_names, columns = ['Accuracy', 'Precision', 'Recall', 'F1'])\n",
    "      \n",
    "    return data_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Accuracy  Precision    Recall        F1\n",
      "related                 0.989755   0.991883  0.994795  0.993337\n",
      "request                 0.987706   0.994088  0.935304  0.963801\n",
      "offer                   0.999129   1.000000  0.808989  0.894410\n",
      "aid_related             0.985042   0.995600  0.968570  0.981899\n",
      "medical_help            0.987091   0.997729  0.841098  0.912742\n",
      "medical_products        0.990830   0.997546  0.821212  0.900831\n",
      "search_and_rescue       0.995338   1.000000  0.828302  0.906089\n",
      "security                0.995748   1.000000  0.764873  0.866774\n",
      "military                0.995236   1.000000  0.861607  0.925659\n",
      "water                   0.995185   0.999133  0.925301  0.960801\n",
      "food                    0.995338   0.997618  0.960550  0.978733\n",
      "shelter                 0.992623   0.999396  0.920511  0.958333\n",
      "clothing                0.998207   0.992647  0.891089  0.939130\n",
      "money                   0.996107   0.997175  0.824766  0.902813\n",
      "missing_people          0.996978   1.000000  0.741228  0.851385\n",
      "refugees                0.994058   1.000000  0.819876  0.901024\n",
      "death                   0.995185   0.997558  0.898790  0.945602\n",
      "other_aid               0.978690   0.997204  0.839216  0.911414\n",
      "infrastructure_related  0.986732   1.000000  0.802742  0.890579\n",
      "transport               0.990830   1.000000  0.801111  0.889574\n",
      "buildings               0.992879   0.997735  0.865422  0.926881\n",
      "electricity             0.995646   1.000000  0.791667  0.883721\n",
      "tools                   0.998566   0.988095  0.754545  0.855670\n",
      "hospitals               0.996414   1.000000  0.677419  0.807692\n",
      "shops                   0.998771   1.000000  0.730337  0.844156\n",
      "aid_centers             0.996260   1.000000  0.703252  0.825776\n",
      "other_infrastructure    0.990574   1.000000  0.791619  0.883692\n",
      "weather_related         0.988115   0.995843  0.961679  0.978463\n",
      "floods                  0.992675   0.996603  0.914019  0.953526\n",
      "storm                   0.994314   0.998865  0.941680  0.969430\n",
      "fire                    0.997695   1.000000  0.777228  0.874652\n",
      "earthquake              0.996465   0.994432  0.968022  0.981049\n",
      "cold                    0.996875   0.997093  0.851117  0.918340\n",
      "other_weather           0.988628   0.998791  0.788921  0.881537\n",
      "direct_report           0.982788   0.997463  0.915416  0.954680\n"
     ]
    }
   ],
   "source": [
    "# Calculate evaluation metrics for training set\n",
    "y_train_pred = pipeline.predict(X_train)\n",
    "col_names = list(y.columns.values)\n",
    "\n",
    "print(eval_metrics(np.array(y_train), y_train_pred, col_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Accuracy  Precision    Recall        F1\n",
      "related                 0.802828   0.838830  0.915041  0.875279\n",
      "request                 0.889196   0.761240  0.464083  0.576629\n",
      "offer                   0.995543   0.000000  0.000000  0.000000\n",
      "aid_related             0.750884   0.745152  0.601565  0.665704\n",
      "medical_help            0.920393   0.495050  0.096712  0.161812\n",
      "medical_products        0.953127   0.714286  0.092879  0.164384\n",
      "search_and_rescue       0.970647   0.588235  0.051546  0.094787\n",
      "security                0.981405   0.000000  0.000000  0.000000\n",
      "military                0.972337   0.600000  0.127660  0.210526\n",
      "water                   0.952205   0.858025  0.325527  0.471986\n",
      "food                    0.931919   0.815126  0.522207  0.636587\n",
      "shelter                 0.936530   0.740566  0.304854  0.431912\n",
      "clothing                0.986783   0.863636  0.186275  0.306452\n",
      "money                   0.973874   1.000000  0.034091  0.065934\n",
      "missing_people          0.988935   0.000000  0.000000  0.000000\n",
      "refugees                0.965114   0.625000  0.043290  0.080972\n",
      "death                   0.962809   0.841270  0.185965  0.304598\n",
      "other_aid               0.862763   0.514851  0.058036  0.104313\n",
      "infrastructure_related  0.937913   0.071429  0.002551  0.004926\n",
      "transport               0.956047   0.666667  0.099668  0.173410\n",
      "buildings               0.955279   0.740000  0.117460  0.202740\n",
      "electricity             0.980329   0.250000  0.016129  0.030303\n",
      "tools                   0.992316   0.000000  0.000000  0.000000\n",
      "hospitals               0.989550   0.000000  0.000000  0.000000\n",
      "shops                   0.995236   0.000000  0.000000  0.000000\n",
      "aid_centers             0.990318   0.000000  0.000000  0.000000\n",
      "other_infrastructure    0.957584   0.000000  0.000000  0.000000\n",
      "weather_related         0.865683   0.837993  0.643368  0.727895\n",
      "floods                  0.944675   0.874016  0.403636  0.552239\n",
      "storm                   0.938220   0.765432  0.432056  0.552339\n",
      "fire                    0.987706   0.000000  0.000000  0.000000\n",
      "earthquake              0.967112   0.876426  0.755738  0.811620\n",
      "cold                    0.981558   0.733333  0.086614  0.154930\n",
      "other_weather           0.949439   0.500000  0.030395  0.057307\n",
      "direct_report           0.852774   0.744639  0.315964  0.443670\n"
     ]
    }
   ],
   "source": [
    "# Calculate evaluation metrics for test set\n",
    "y_test_pred = pipeline.predict(X_test)\n",
    "\n",
    "eval_metrics0 = eval_metrics(np.array(y_test), y_test_pred, col_names)\n",
    "print(eval_metrics0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=<function tokenize at 0x7f94da3d0400>, vocabulary=None)),\n",
       "  ('tfidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       "  ('clf',\n",
       "   MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "               max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "               oob_score=False, random_state=None, verbose=0,\n",
       "               warm_start=False),\n",
       "              n_jobs=1))],\n",
       " 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=<function tokenize at 0x7f94da3d0400>, vocabulary=None),\n",
       " 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'clf': MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "             oob_score=False, random_state=None, verbose=0,\n",
       "             warm_start=False),\n",
       "            n_jobs=1),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': <function __main__.tokenize(text)>,\n",
       " 'vect__vocabulary': None,\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': True,\n",
       " 'clf__estimator__bootstrap': True,\n",
       " 'clf__estimator__class_weight': None,\n",
       " 'clf__estimator__criterion': 'gini',\n",
       " 'clf__estimator__max_depth': None,\n",
       " 'clf__estimator__max_features': 'auto',\n",
       " 'clf__estimator__max_leaf_nodes': None,\n",
       " 'clf__estimator__min_impurity_decrease': 0.0,\n",
       " 'clf__estimator__min_impurity_split': None,\n",
       " 'clf__estimator__min_samples_leaf': 1,\n",
       " 'clf__estimator__min_samples_split': 2,\n",
       " 'clf__estimator__min_weight_fraction_leaf': 0.0,\n",
       " 'clf__estimator__n_estimators': 10,\n",
       " 'clf__estimator__n_jobs': 1,\n",
       " 'clf__estimator__oob_score': False,\n",
       " 'clf__estimator__random_state': None,\n",
       " 'clf__estimator__verbose': 0,\n",
       " 'clf__estimator__warm_start': False,\n",
       " 'clf__estimator': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "             oob_score=False, random_state=None, verbose=0,\n",
       "             warm_start=False),\n",
       " 'clf__n_jobs': 1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Review params to get ideas to improve model\n",
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 8 candidates, totalling 16 fits\n",
      "[CV] clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.22702591947546358, total= 2.0min\n",
      "[CV] clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.8min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.23524590163934425, total= 2.0min\n",
      "[CV] clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  5.6min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.23184099989755147, total= 2.4min\n",
      "[CV] clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.23063524590163934, total= 2.4min\n",
      "[CV] clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.22948468394631696, total= 2.0min\n",
      "[CV] clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.23237704918032787, total= 2.0min\n",
      "[CV] clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.22938223542669808, total= 2.5min\n",
      "[CV] clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.22172131147540983, total= 2.5min\n",
      "[CV] clf__estimator__n_estimators=20, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=20, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.24188095482020286, total= 2.4min\n",
      "[CV] clf__estimator__n_estimators=20, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=20, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.23965163934426228, total= 2.4min\n",
      "[CV] clf__estimator__n_estimators=20, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=20, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.24433971929105625, total= 3.4min\n",
      "[CV] clf__estimator__n_estimators=20, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=20, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.2403688524590164, total= 3.4min\n",
      "[CV] clf__estimator__n_estimators=20, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=20, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.2373732199569716, total= 2.4min\n",
      "[CV] clf__estimator__n_estimators=20, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=20, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.2421106557377049, total= 2.5min\n",
      "[CV] clf__estimator__n_estimators=20, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=20, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.24321278557524845, total= 3.4min\n",
      "[CV] clf__estimator__n_estimators=20, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=20, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.244672131147541, total= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  16 out of  16 | elapsed: 55.2min finished\n"
     ]
    }
   ],
   "source": [
    "# Perform grid search to find better parameters\n",
    "parameters = {'tfidf__use_idf': (True, False),\n",
    "              'vect__ngram_range': ((1, 1), (1, 2)),\n",
    "              'clf__estimator__n_estimators': [10,20],\n",
    "}\n",
    "cv = GridSearchCV(estimator=pipeline, param_grid=parameters, cv=2, n_jobs=-1, verbose=3)\n",
    "\n",
    "# Find best parameters\n",
    "T_model = cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([  69.11949754,   94.43226147,   70.08173239,   97.21860027,\n",
       "          90.94400918,  149.30103743,   93.33645368,  151.88576055]),\n",
       " 'std_fit_time': array([ 0.69949532,  0.57339287,  1.01761878,  0.00297737,  0.2424165 ,\n",
       "         1.02877223,  0.20926046,  0.86911058]),\n",
       " 'mean_score_time': array([ 49.76156354,  50.82567406,  50.49769127,  51.38450336,\n",
       "         52.66521215,  55.63835096,  52.94630396,  55.47585034]),\n",
       " 'std_score_time': array([ 1.16416526,  1.09567046,  0.69959939,  1.04526377,  1.41914487,\n",
       "         0.26248193,  1.48756683,  1.53438902]),\n",
       " 'param_clf__estimator__n_estimators': masked_array(data = [10 10 10 10 20 20 20 20],\n",
       "              mask = [False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_tfidf__use_idf': masked_array(data = [True True False False True True False False],\n",
       "              mask = [False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_vect__ngram_range': masked_array(data = [(1, 1) (1, 2) (1, 1) (1, 2) (1, 1) (1, 2) (1, 1) (1, 2)],\n",
       "              mask = [False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'params': [{'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__ngram_range': (1, 1)},\n",
       "  {'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__ngram_range': (1, 2)},\n",
       "  {'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__ngram_range': (1, 1)},\n",
       "  {'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__ngram_range': (1, 2)},\n",
       "  {'clf__estimator__n_estimators': 20,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__ngram_range': (1, 1)},\n",
       "  {'clf__estimator__n_estimators': 20,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__ngram_range': (1, 2)},\n",
       "  {'clf__estimator__n_estimators': 20,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__ngram_range': (1, 1)},\n",
       "  {'clf__estimator__n_estimators': 20,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__ngram_range': (1, 2)}],\n",
       " 'split0_test_score': array([ 0.22702592,  0.231841  ,  0.22948468,  0.22938224,  0.24188095,\n",
       "         0.24433972,  0.23737322,  0.24321279]),\n",
       " 'split1_test_score': array([ 0.2352459 ,  0.23063525,  0.23237705,  0.22172131,  0.23965164,\n",
       "         0.24036885,  0.24211066,  0.24467213]),\n",
       " 'mean_test_score': array([ 0.2311357 ,  0.23123815,  0.23093079,  0.22555197,  0.24076635,\n",
       "         0.24235439,  0.23974182,  0.24394242]),\n",
       " 'std_test_score': array([ 0.00410999,  0.00060288,  0.00144618,  0.00383046,  0.00111466,\n",
       "         0.00198543,  0.00236872,  0.00072967]),\n",
       " 'rank_test_score': array([6, 5, 7, 8, 3, 2, 4, 1], dtype=int32),\n",
       " 'split0_train_score': array([ 0.80819672,  0.78862705,  0.81536885,  0.78596311,  0.90881148,\n",
       "         0.89170082,  0.9170082 ,  0.89620902]),\n",
       " 'split1_train_score': array([ 0.80749923,  0.78516545,  0.80370864,  0.77901854,  0.90871837,\n",
       "         0.89304375,  0.90810368,  0.8929413 ]),\n",
       " 'mean_train_score': array([ 0.80784798,  0.78689625,  0.80953874,  0.78249083,  0.90876492,\n",
       "         0.89237228,  0.91255594,  0.89457516]),\n",
       " 'std_train_score': array([  3.48744838e-04,   1.73079741e-03,   5.83010802e-03,\n",
       "          3.47228579e-03,   4.65531951e-05,   6.71462923e-04,\n",
       "          4.45225941e-03,   1.63385970e-03])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grid search results\n",
    "T_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24394242098253163"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top mean score\n",
    "np.max(T_model.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__estimator__n_estimators': 20,\n",
       " 'tfidf__use_idf': False,\n",
       " 'vect__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters for top mean score\n",
    "T_model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Accuracy  Precision    Recall        F1\n",
      "related                 0.814661   0.837636  0.936382  0.884261\n",
      "request                 0.898417   0.816587  0.483932  0.607715\n",
      "offer                   0.995543   0.000000  0.000000  0.000000\n",
      "aid_related             0.764561   0.778423  0.599702  0.677474\n",
      "medical_help            0.921777   0.547619  0.088975  0.153078\n",
      "medical_products        0.953281   0.771429  0.083591  0.150838\n",
      "search_and_rescue       0.970647   0.615385  0.041237  0.077295\n",
      "security                0.981866   0.500000  0.008475  0.016667\n",
      "military                0.971108   0.500000  0.063830  0.113208\n",
      "water                   0.955433   0.870270  0.377049  0.526144\n",
      "food                    0.932227   0.881313  0.469717  0.612818\n",
      "shelter                 0.935761   0.793939  0.254369  0.385294\n",
      "clothing                0.985861   0.812500  0.127451  0.220339\n",
      "money                   0.973567   1.000000  0.022727  0.044444\n",
      "missing_people          0.989089   0.000000  0.000000  0.000000\n",
      "refugees                0.964961   0.615385  0.034632  0.065574\n",
      "death                   0.960043   0.857143  0.105263  0.187500\n",
      "other_aid               0.865529   0.677966  0.044643  0.083770\n",
      "infrastructure_related  0.939142   0.250000  0.005102  0.010000\n",
      "transport               0.953742   0.500000  0.019934  0.038339\n",
      "buildings               0.954972   0.729167  0.111111  0.192837\n",
      "electricity             0.980790   0.333333  0.008065  0.015748\n",
      "tools                   0.992316   0.000000  0.000000  0.000000\n",
      "hospitals               0.989857   0.000000  0.000000  0.000000\n",
      "shops                   0.995236   0.000000  0.000000  0.000000\n",
      "aid_centers             0.990164   0.000000  0.000000  0.000000\n",
      "other_infrastructure    0.958353   0.000000  0.000000  0.000000\n",
      "weather_related         0.865068   0.855951  0.621354  0.720026\n",
      "floods                  0.944368   0.898305  0.385455  0.539440\n",
      "storm                   0.938528   0.782468  0.419861  0.546485\n",
      "fire                    0.987859   1.000000  0.012500  0.024691\n",
      "earthquake              0.968957   0.879182  0.775410  0.824042\n",
      "cold                    0.980790   0.750000  0.023622  0.045802\n",
      "other_weather           0.950515   0.705882  0.036474  0.069364\n",
      "direct_report           0.857845   0.772031  0.333333  0.465627\n"
     ]
    }
   ],
   "source": [
    "# Evaluate metrics for test set\n",
    "tuned_pred_test = T_model.predict(X_test)\n",
    "\n",
    "eval_metrics1 = eval_metrics(np.array(y_test), tuned_pred_test, col_names)\n",
    "\n",
    "print(eval_metrics1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.943972</td>\n",
       "      <td>0.516034</td>\n",
       "      <td>0.197523</td>\n",
       "      <td>0.253236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.056416</td>\n",
       "      <td>0.353965</td>\n",
       "      <td>0.248534</td>\n",
       "      <td>0.272337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.750884</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.937221</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.001276</td>\n",
       "      <td>0.002463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.957584</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.092879</td>\n",
       "      <td>0.161812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.790279</td>\n",
       "      <td>0.320745</td>\n",
       "      <td>0.457828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.995543</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.915041</td>\n",
       "      <td>0.875279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Accuracy  Precision     Recall         F1\n",
       "count  35.000000  35.000000  35.000000  35.000000\n",
       "mean    0.943972   0.516034   0.197523   0.253236\n",
       "std     0.056416   0.353965   0.248534   0.272337\n",
       "min     0.750884   0.000000   0.000000   0.000000\n",
       "25%     0.937221   0.035714   0.001276   0.002463\n",
       "50%     0.957584   0.666667   0.092879   0.161812\n",
       "75%     0.981481   0.790279   0.320745   0.457828\n",
       "max     0.995543   1.000000   0.915041   0.875279"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary of first model\n",
    "eval_metrics0.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.945224</td>\n",
       "      <td>0.580912</td>\n",
       "      <td>0.185548</td>\n",
       "      <td>0.237109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.053560</td>\n",
       "      <td>0.337445</td>\n",
       "      <td>0.253397</td>\n",
       "      <td>0.279496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.764561</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.937145</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.008270</td>\n",
       "      <td>0.016207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.958353</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>0.044643</td>\n",
       "      <td>0.083770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.981328</td>\n",
       "      <td>0.827112</td>\n",
       "      <td>0.355191</td>\n",
       "      <td>0.495885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.995543</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.936382</td>\n",
       "      <td>0.884261</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Accuracy  Precision     Recall         F1\n",
       "count  35.000000  35.000000  35.000000  35.000000\n",
       "mean    0.945224   0.580912   0.185548   0.237109\n",
       "std     0.053560   0.337445   0.253397   0.279496\n",
       "min     0.764561   0.000000   0.000000   0.000000\n",
       "25%     0.937145   0.416667   0.008270   0.016207\n",
       "50%     0.958353   0.729167   0.044643   0.083770\n",
       "75%     0.981328   0.827112   0.355191   0.495885\n",
       "max     0.995543   1.000000   0.936382   0.884261"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary of tuned model\n",
    "eval_metrics1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improve the pipeline\n",
    "pipeline2 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('best', TruncatedSVD()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(AdaBoostClassifier()))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...mator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None),\n",
       "           n_jobs=1))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# Train pipeline 2\n",
    "pipeline2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Accuracy  Precision    Recall        F1\n",
      "related                 0.764561   0.764646  0.999464  0.866426\n",
      "request                 0.826392   1.000000  0.001179  0.002355\n",
      "offer                   0.996107   1.000000  0.025641  0.050000\n",
      "aid_related             0.582859   0.575540  0.009799  0.019270\n",
      "medical_help            0.919062   1.000000  0.001264  0.002525\n",
      "medical_products        0.948978   1.000000  0.001003  0.002004\n",
      "search_and_rescue       0.972030   1.000000  0.005464  0.010870\n",
      "security                0.982019   1.000000  0.002841  0.005666\n",
      "military                0.966959   1.000000  0.001548  0.003091\n",
      "water                   0.935710   1.000000  0.000796  0.001591\n",
      "food                    0.886379   1.000000  0.000901  0.001800\n",
      "shelter                 0.910609   1.000000  0.003427  0.006830\n",
      "clothing                0.983915   1.000000  0.006329  0.012579\n",
      "money                   0.976538   1.000000  0.004348  0.008658\n",
      "missing_people          0.989242   1.000000  0.004739  0.009434\n",
      "refugees                0.965678   1.000000  0.002976  0.005935\n",
      "death                   0.954511   1.000000  0.002247  0.004484\n",
      "other_aid               0.867732   1.000000  0.001161  0.002318\n",
      "infrastructure_related  0.933405   1.000000  0.001536  0.003067\n",
      "transport               0.954818   0.750000  0.003394  0.006757\n",
      "buildings               0.949388   1.000000  0.002020  0.004032\n",
      "electricity             0.979304   1.000000  0.004926  0.009804\n",
      "tools                   0.994109   1.000000  0.017094  0.033613\n",
      "hospitals               0.988935   1.000000  0.009174  0.018182\n",
      "shops                   0.995492   1.000000  0.022222  0.043478\n",
      "aid_centers             0.988013   1.000000  0.004255  0.008475\n",
      "other_infrastructure    0.954972   1.000000  0.002270  0.004530\n",
      "weather_related         0.721121   0.725000  0.005309  0.010542\n",
      "floods                  0.917473   1.000000  0.001859  0.003711\n",
      "storm                   0.906306   1.000000  0.001092  0.002182\n",
      "fire                    0.989140   1.000000  0.009346  0.018519\n",
      "earthquake              0.905794   1.000000  0.001086  0.002170\n",
      "cold                    0.980022   1.000000  0.005102  0.010152\n",
      "other_weather           0.948312   1.000000  0.000990  0.001978\n",
      "direct_report           0.804877   0.727273  0.002098  0.004183\n"
     ]
    }
   ],
   "source": [
    "# Calculate evaluation metrics for training set 2\n",
    "y_train_pred2 = pipeline2.predict(X_train)\n",
    "col_names_ = list(y.columns.values)\n",
    "\n",
    "print(eval_metrics(np.array(y_train), y_train_pred2, col_names_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Accuracy  Precision    Recall        F1\n",
      "related                 0.768403   0.768697  0.998598  0.868694\n",
      "request                 0.833871   0.000000  0.000000  0.000000\n",
      "offer                   0.993853   0.000000  0.000000  0.000000\n",
      "aid_related             0.585062   0.454545  0.007418  0.014599\n",
      "medical_help            0.922852   0.000000  0.000000  0.000000\n",
      "medical_products        0.951283   0.000000  0.000000  0.000000\n",
      "search_and_rescue       0.972645   0.000000  0.000000  0.000000\n",
      "security                0.981405   0.000000  0.000000  0.000000\n",
      "military                0.967112   0.000000  0.000000  0.000000\n",
      "water                   0.936069   0.000000  0.000000  0.000000\n",
      "food                    0.891963   0.000000  0.000000  0.000000\n",
      "shelter                 0.913478   0.000000  0.000000  0.000000\n",
      "clothing                0.986322   0.000000  0.000000  0.000000\n",
      "money                   0.977870   0.000000  0.000000  0.000000\n",
      "missing_people          0.986476   0.000000  0.000000  0.000000\n",
      "refugees                0.968649   0.000000  0.000000  0.000000\n",
      "death                   0.953281   0.000000  0.000000  0.000000\n",
      "other_aid               0.867681   0.000000  0.000000  0.000000\n",
      "infrastructure_related  0.938067   0.000000  0.000000  0.000000\n",
      "transport               0.951283   0.000000  0.000000  0.000000\n",
      "buildings               0.947134   0.000000  0.000000  0.000000\n",
      "electricity             0.980329   0.000000  0.000000  0.000000\n",
      "tools                   0.993545   0.000000  0.000000  0.000000\n",
      "hospitals               0.990011   0.000000  0.000000  0.000000\n",
      "shops                   0.995236   0.000000  0.000000  0.000000\n",
      "aid_centers             0.988628   0.000000  0.000000  0.000000\n",
      "other_infrastructure    0.958506   0.000000  0.000000  0.000000\n",
      "weather_related         0.717996   0.500000  0.003815  0.007572\n",
      "floods                  0.916859   0.000000  0.000000  0.000000\n",
      "storm                   0.905947   0.000000  0.000000  0.000000\n",
      "fire                    0.989242   0.000000  0.000000  0.000000\n",
      "earthquake              0.905333   0.000000  0.000000  0.000000\n",
      "cold                    0.978792   0.000000  0.000000  0.000000\n",
      "other_weather           0.943599   0.000000  0.000000  0.000000\n",
      "direct_report           0.805133   0.111111  0.000793  0.001575\n"
     ]
    }
   ],
   "source": [
    "# Calculate evaluation metrics for test set 2\n",
    "y_test_pred2 = pipeline2.predict(X_test)\n",
    "\n",
    "eval_metrics2 = eval_metrics(np.array(y_test), y_test_pred2, col_names)\n",
    "print(eval_metrics2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None)),\n",
       "  ('best', TruncatedSVD(algorithm='randomized', n_components=2, n_iter=5,\n",
       "          random_state=None, tol=0.0)),\n",
       "  ('tfidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       "  ('clf',\n",
       "   MultiOutputClassifier(estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "             learning_rate=1.0, n_estimators=50, random_state=None),\n",
       "              n_jobs=1))],\n",
       " 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       " 'best': TruncatedSVD(algorithm='randomized', n_components=2, n_iter=5,\n",
       "        random_state=None, tol=0.0),\n",
       " 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'clf': MultiOutputClassifier(estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "           learning_rate=1.0, n_estimators=50, random_state=None),\n",
       "            n_jobs=1),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': None,\n",
       " 'vect__vocabulary': None,\n",
       " 'best__algorithm': 'randomized',\n",
       " 'best__n_components': 2,\n",
       " 'best__n_iter': 5,\n",
       " 'best__random_state': None,\n",
       " 'best__tol': 0.0,\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': True,\n",
       " 'clf__estimator__algorithm': 'SAMME.R',\n",
       " 'clf__estimator__base_estimator': None,\n",
       " 'clf__estimator__learning_rate': 1.0,\n",
       " 'clf__estimator__n_estimators': 50,\n",
       " 'clf__estimator__random_state': None,\n",
       " 'clf__estimator': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "           learning_rate=1.0, n_estimators=50, random_state=None),\n",
       " 'clf__n_jobs': 1}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at parameters for improving model\n",
    "pipeline2.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 8 candidates, totalling 16 fits\n",
      "[CV] clf__estimator__learning_rate=1, clf__estimator__n_estimators=50, tfidf__use_idf=True \n",
      "[CV]  clf__estimator__learning_rate=1, clf__estimator__n_estimators=50, tfidf__use_idf=True, score=0.18676365126523922, total=  15.1s\n",
      "[CV] clf__estimator__learning_rate=1, clf__estimator__n_estimators=50, tfidf__use_idf=True \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   17.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__learning_rate=1, clf__estimator__n_estimators=50, tfidf__use_idf=True, score=0.18493852459016394, total=  16.0s\n",
      "[CV] clf__estimator__learning_rate=1, clf__estimator__n_estimators=50, tfidf__use_idf=False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   35.9s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__learning_rate=1, clf__estimator__n_estimators=50, tfidf__use_idf=False, score=0.18010449749001126, total=  14.8s\n",
      "[CV] clf__estimator__learning_rate=1, clf__estimator__n_estimators=50, tfidf__use_idf=False \n",
      "[CV]  clf__estimator__learning_rate=1, clf__estimator__n_estimators=50, tfidf__use_idf=False, score=0.18370901639344261, total=  15.1s\n",
      "[CV] clf__estimator__learning_rate=1, clf__estimator__n_estimators=100, tfidf__use_idf=True \n",
      "[CV]  clf__estimator__learning_rate=1, clf__estimator__n_estimators=100, tfidf__use_idf=True, score=0.18276815900010246, total=  29.3s\n",
      "[CV] clf__estimator__learning_rate=1, clf__estimator__n_estimators=100, tfidf__use_idf=True \n",
      "[CV]  clf__estimator__learning_rate=1, clf__estimator__n_estimators=100, tfidf__use_idf=True, score=0.18299180327868853, total=  29.4s\n",
      "[CV] clf__estimator__learning_rate=1, clf__estimator__n_estimators=100, tfidf__use_idf=False \n",
      "[CV]  clf__estimator__learning_rate=1, clf__estimator__n_estimators=100, tfidf__use_idf=False, score=0.1847146808728614, total=  29.0s\n",
      "[CV] clf__estimator__learning_rate=1, clf__estimator__n_estimators=100, tfidf__use_idf=False \n",
      "[CV]  clf__estimator__learning_rate=1, clf__estimator__n_estimators=100, tfidf__use_idf=False, score=0.1817622950819672, total=  29.2s\n",
      "[CV] clf__estimator__learning_rate=2, clf__estimator__n_estimators=50, tfidf__use_idf=True \n",
      "[CV]  clf__estimator__learning_rate=2, clf__estimator__n_estimators=50, tfidf__use_idf=True, score=0.0, total=  20.9s\n",
      "[CV] clf__estimator__learning_rate=2, clf__estimator__n_estimators=50, tfidf__use_idf=True \n",
      "[CV]  clf__estimator__learning_rate=2, clf__estimator__n_estimators=50, tfidf__use_idf=True, score=0.0, total=  20.5s\n",
      "[CV] clf__estimator__learning_rate=2, clf__estimator__n_estimators=50, tfidf__use_idf=False \n",
      "[CV]  clf__estimator__learning_rate=2, clf__estimator__n_estimators=50, tfidf__use_idf=False, score=0.0, total=  21.6s\n",
      "[CV] clf__estimator__learning_rate=2, clf__estimator__n_estimators=50, tfidf__use_idf=False \n",
      "[CV]  clf__estimator__learning_rate=2, clf__estimator__n_estimators=50, tfidf__use_idf=False, score=0.0, total=  20.2s\n",
      "[CV] clf__estimator__learning_rate=2, clf__estimator__n_estimators=100, tfidf__use_idf=True \n",
      "[CV]  clf__estimator__learning_rate=2, clf__estimator__n_estimators=100, tfidf__use_idf=True, score=0.0, total=  56.5s\n",
      "[CV] clf__estimator__learning_rate=2, clf__estimator__n_estimators=100, tfidf__use_idf=True \n",
      "[CV]  clf__estimator__learning_rate=2, clf__estimator__n_estimators=100, tfidf__use_idf=True, score=0.0, total=  49.1s\n",
      "[CV] clf__estimator__learning_rate=2, clf__estimator__n_estimators=100, tfidf__use_idf=False \n",
      "[CV]  clf__estimator__learning_rate=2, clf__estimator__n_estimators=100, tfidf__use_idf=False, score=0.0, total=  53.4s\n",
      "[CV] clf__estimator__learning_rate=2, clf__estimator__n_estimators=100, tfidf__use_idf=False \n",
      "[CV]  clf__estimator__learning_rate=2, clf__estimator__n_estimators=100, tfidf__use_idf=False, score=0.0, total=  50.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  16 out of  16 | elapsed:  9.7min finished\n"
     ]
    }
   ],
   "source": [
    "# Model tuning with added parameters\n",
    "parameters2 = {'tfidf__use_idf': (True, False), \n",
    "               'clf__estimator__n_estimators': [50, 100],\n",
    "               'clf__estimator__learning_rate': [1,2]\n",
    "              }\n",
    "cv2 = GridSearchCV(estimator=pipeline2, param_grid=parameters2, cv=2, n_jobs=-1, verbose=3)\n",
    "\n",
    "# Find best parameters\n",
    "T_model2 = cv2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1858511346754777"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top mean score for model 2\n",
    "np.max(T_model2.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__estimator__learning_rate': 1,\n",
       " 'clf__estimator__n_estimators': 50,\n",
       " 'tfidf__use_idf': True}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters for top mean score model 2\n",
    "T_model2.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Accuracy  Precision    Recall        F1\n",
      "related                 0.768403   0.768697  0.998598  0.868694\n",
      "request                 0.833871   0.000000  0.000000  0.000000\n",
      "offer                   0.993853   0.000000  0.000000  0.000000\n",
      "aid_related             0.583525   0.464286  0.033754  0.062932\n",
      "medical_help            0.922852   0.000000  0.000000  0.000000\n",
      "medical_products        0.951283   0.000000  0.000000  0.000000\n",
      "search_and_rescue       0.972645   0.000000  0.000000  0.000000\n",
      "security                0.981405   0.000000  0.000000  0.000000\n",
      "military                0.967112   0.000000  0.000000  0.000000\n",
      "water                   0.936069   0.000000  0.000000  0.000000\n",
      "food                    0.891963   0.000000  0.000000  0.000000\n",
      "shelter                 0.913478   0.000000  0.000000  0.000000\n",
      "clothing                0.986169   0.000000  0.000000  0.000000\n",
      "money                   0.977870   0.000000  0.000000  0.000000\n",
      "missing_people          0.986476   0.000000  0.000000  0.000000\n",
      "refugees                0.968495   0.000000  0.000000  0.000000\n",
      "death                   0.953281   0.000000  0.000000  0.000000\n",
      "other_aid               0.867681   0.000000  0.000000  0.000000\n",
      "infrastructure_related  0.938067   0.000000  0.000000  0.000000\n",
      "transport               0.951283   0.000000  0.000000  0.000000\n",
      "buildings               0.947134   0.000000  0.000000  0.000000\n",
      "electricity             0.980329   0.000000  0.000000  0.000000\n",
      "tools                   0.993545   0.000000  0.000000  0.000000\n",
      "hospitals               0.990011   0.000000  0.000000  0.000000\n",
      "shops                   0.995236   0.000000  0.000000  0.000000\n",
      "aid_centers             0.988628   0.000000  0.000000  0.000000\n",
      "other_infrastructure    0.958506   0.000000  0.000000  0.000000\n",
      "weather_related         0.717996   0.500000  0.003815  0.007572\n",
      "floods                  0.916551   0.000000  0.000000  0.000000\n",
      "storm                   0.905794   0.000000  0.000000  0.000000\n",
      "fire                    0.989242   0.000000  0.000000  0.000000\n",
      "earthquake              0.905333   0.000000  0.000000  0.000000\n",
      "cold                    0.978792   0.000000  0.000000  0.000000\n",
      "other_weather           0.943599   0.000000  0.000000  0.000000\n",
      "direct_report           0.805133   0.111111  0.000793  0.001575\n"
     ]
    }
   ],
   "source": [
    "# Evaluating metrics for test set model 2\n",
    "tuned_pred_test2 = T_model2.predict(X_test)\n",
    "\n",
    "eval_metrics2 = eval_metrics(np.array(y_test), tuned_pred_test2, col_names)\n",
    "\n",
    "print(eval_metrics2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.943972</td>\n",
       "      <td>0.516034</td>\n",
       "      <td>0.197523</td>\n",
       "      <td>0.253236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.056416</td>\n",
       "      <td>0.353965</td>\n",
       "      <td>0.248534</td>\n",
       "      <td>0.272337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.750884</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.937221</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.001276</td>\n",
       "      <td>0.002463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.957584</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.092879</td>\n",
       "      <td>0.161812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.790279</td>\n",
       "      <td>0.320745</td>\n",
       "      <td>0.457828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.995543</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.915041</td>\n",
       "      <td>0.875279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Accuracy  Precision     Recall         F1\n",
       "count  35.000000  35.000000  35.000000  35.000000\n",
       "mean    0.943972   0.516034   0.197523   0.253236\n",
       "std     0.056416   0.353965   0.248534   0.272337\n",
       "min     0.750884   0.000000   0.000000   0.000000\n",
       "25%     0.937221   0.035714   0.001276   0.002463\n",
       "50%     0.957584   0.666667   0.092879   0.161812\n",
       "75%     0.981481   0.790279   0.320745   0.457828\n",
       "max     0.995543   1.000000   0.915041   0.875279"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary of original model\n",
    "eval_metrics0.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.945224</td>\n",
       "      <td>0.580912</td>\n",
       "      <td>0.185548</td>\n",
       "      <td>0.237109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.053560</td>\n",
       "      <td>0.337445</td>\n",
       "      <td>0.253397</td>\n",
       "      <td>0.279496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.764561</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.937145</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.008270</td>\n",
       "      <td>0.016207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.958353</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>0.044643</td>\n",
       "      <td>0.083770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.981328</td>\n",
       "      <td>0.827112</td>\n",
       "      <td>0.355191</td>\n",
       "      <td>0.495885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.995543</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.936382</td>\n",
       "      <td>0.884261</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Accuracy  Precision     Recall         F1\n",
       "count  35.000000  35.000000  35.000000  35.000000\n",
       "mean    0.945224   0.580912   0.185548   0.237109\n",
       "std     0.053560   0.337445   0.253397   0.279496\n",
       "min     0.764561   0.000000   0.000000   0.000000\n",
       "25%     0.937145   0.416667   0.008270   0.016207\n",
       "50%     0.958353   0.729167   0.044643   0.083770\n",
       "75%     0.981328   0.827112   0.355191   0.495885\n",
       "max     0.995543   1.000000   0.936382   0.884261"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary of tuned model 1\n",
    "eval_metrics1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.924617</td>\n",
       "      <td>0.052688</td>\n",
       "      <td>0.029627</td>\n",
       "      <td>0.026879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.088879</td>\n",
       "      <td>0.169049</td>\n",
       "      <td>0.168700</td>\n",
       "      <td>0.146865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.583525</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.909636</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.951283</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.980867</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.995236</td>\n",
       "      <td>0.768697</td>\n",
       "      <td>0.998598</td>\n",
       "      <td>0.868694</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Accuracy  Precision     Recall         F1\n",
       "count  35.000000  35.000000  35.000000  35.000000\n",
       "mean    0.924617   0.052688   0.029627   0.026879\n",
       "std     0.088879   0.169049   0.168700   0.146865\n",
       "min     0.583525   0.000000   0.000000   0.000000\n",
       "25%     0.909636   0.000000   0.000000   0.000000\n",
       "50%     0.951283   0.000000   0.000000   0.000000\n",
       "75%     0.980867   0.000000   0.000000   0.000000\n",
       "max     0.995236   0.768697   0.998598   0.868694"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary of tuned model 2\n",
    "eval_metrics2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Tuned Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Model 1 has higher mean in all categories (accuracy, precision, recall, F1 value). The standard deviation for model 1 is higher, indicating a greater distribution in relation to the mean values\n",
    "* Model 2 has lower mean value for all categories and smaller standard deviation for each metric when compared to model 1\n",
    "\n",
    "* Comparing metrics (mean value) for model 1 and model 2\n",
    ">* **Accuracy**: Both model 1 and model 2 have pretty high accuracy (0.942185 and 0.925368, respectively)\n",
    ">* **Precision**: Model 1 has a higher average precision of 0.559671 when compared to model 2 (0.081468). The higher precision of model 1, indicates that this model is better at predicting actual positives and is better for predicting when the costs of a false positive is high.\n",
    ">* **Recall**: Model 1 has a higer recall of 0.171154 when compared to model 2's recall of 0.030304. The higher recall value in model 1, calculates how many of the actual positives the model captures that are a true positive. This means model 1 would be best selected when there's a high cost associated with false negatives. \n",
    ">* **F1**: Model 1 has a higher F1 value of 0.225254 when compared to model 2 (0.028043). The F1 score is best used if we're seeking to find a balance between Precision and Recall and if there's an uneven distribution. An F1 score of 1 is considered a 'perfect' model, while and F1 score of 0 indicates a bad model. The higher F1 value for model 1 is indicative that the model has low false positives and low false negatives, so it's more effective at correctly identifying real threats and isn't disturbed by false alarms. \n",
    "\n",
    "* Based on the metrics, Model 1 is the best overall model and will be saved for continuation of this work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export best model as pickle file\n",
    "pickle.dump(T_model, open('classifer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources\n",
    "* https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9\n",
    "* https://pathmind.com/wiki/accuracy-precision-recall-f1#:~:text=That%20is%2C%20a%20good%20F1,total%20failure%20when%20it's%200%20\n",
    "* https://apapiu.github.io/2016-08-04-tf_idf/\n",
    "* https://www.greenbook.org/marketing-research/how-to-interpret-standard-deviation-and-standard-error-in-survey-research-03377\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
